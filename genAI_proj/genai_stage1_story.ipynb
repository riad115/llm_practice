{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b21f998",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#init\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b038de67",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test tokenizer\n",
    "#tokenizer(\"The first computer program that is programmed always says: Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de1a30e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Pyro-X2/casual_lm_model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "# checkpoint = \"openai-community/gpt2-medium\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106c14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer(\"The first computer program that is programmed always says: Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d238399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The first computer program that is programmed always says: Hello\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=3)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef8b701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bdf2023f9a48bc8bf2a27c5da9f7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc732b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [{\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
    "{\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"},]\n",
    "tokenizer.chat_template\n",
    "#tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\"\n",
    "# tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a753902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.</s> \n",
      "<|user|>\n",
      "Hey, can you tell me any fun things to do in New York?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb01675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokenized_chat, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=5, max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25153ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.</s> \n",
      "<|user|>\n",
      "Hey, can you tell me any fun things to do in New York?</s> \n",
      "<|assistant|>\n",
      "Sure thing! New York City, also known as the Big Apple, has plenty of fun things to do. Some popular activities include:\n",
      "\n",
      "1. Explore Central Park: Central Park is a vast green space in the heart of Manhattan. You can stroll, bike, or take a horse-drawn carriage ride through its winding paths, enjoy a picnic, or visit the Central Park Zoo.\n",
      "\n",
      "2. Visit the Empire State Building: No trip to New York would be complete without seeing the iconic Empire State Building. Go to the top and enjoy the breathtaking view of the city skyline.\n",
      "\n",
      "3. Check out Times Square: Times Square is a bustling area in Midtown Manhattan, known for its bright lights, billboards, and crowded streets. You'll find plenty of shops, theaters, and restaurants here.\n",
      "\n",
      "4. Walk across the Brooklyn Bridge: The Brooklyn Bridge connects Manhattan and Brooklyn. Walking across the bridge offers a stunning view of the Manhattan skyline and the East River.\n",
      "\n",
      "5. Take a ferry to the Statue of Liberty: The Statue of Liberty is a symbol of freedom and a must-see when you're in New York. You can take a ferry to Liberty Island and see the statue up close.\n",
      "\n",
      "6. Visit the 9/11 Memorial: The 9/11 Memorial is a moving tribute to the victims of the terrorist attacks of September 11, 2001. You can visit the memorials and learn more about the history of the site.\n",
      "\n",
      "7. Try some NYC pizza: New York-style pizza is famous around the world. You can find many pizza places in the city, serving up delicious slices of cheesy goodness.\n",
      "\n",
      "8. Go to a Broadway show: New York is home to some of the best theaters in the world, and Broadway productions are world-renowned. See a show at one of the legendary theaters.\n",
      "\n",
      "These are just a few ideas to get you started. New York has so much to offer, and you're sure to find something that suits your tastes. Enjoy your trip!</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.decode(output[0])\n",
    "print(response)\n",
    "tokenizer\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e134e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "#response = response.strip('\\n')\n",
    "def process_response(chat_response):\n",
    "    response_first_split = chat_response.split('</s>')\n",
    "    #print(response_first_split)\n",
    "    role=\"\"\n",
    "    content=\"\"\n",
    "    for rsp in response_first_split:\n",
    "        for rsps in rsp.split('>'):\n",
    "            if '|' in rsps:\n",
    "                role = rsps.split('|')[-2]\n",
    "    #             print(role)\n",
    "            else:\n",
    "                content = rsps.strip()\n",
    "    #             print(content)\n",
    "    #chat.append({\"role\":role,\"content\":content})\n",
    "#     print(\"role: \"+role+\" content: \"+content)\n",
    "    return role, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "chat_history = chat[:]\n",
    "role, content = process_response(response)\n",
    "chat_history.append({\"role\":role,\"content\":content})\n",
    "#print(chat_history)\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     print(input(\">> User:\"))\n",
    "#     user_ask = [{\"role\":\"user\", \"content\": input(\"content: \")}]\n",
    "#     print(user_ask)\n",
    "    chat_history.append({\"role\":\"user\", \"content\": input(\"content: \")})\n",
    "#     print(chat_history)\n",
    "    tokenized_chat_history = tokenizer.apply_chat_template(chat_history, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     print(tokenizer.decode(tokenized_chat[0]))\n",
    "    output_history = model.generate(tokenized_chat_history, max_new_tokens=256, pad_token_id=tokenizer.eos_token_id)\n",
    "    response_history = tokenizer.decode(output_history[0])\n",
    "    print(response_history)\n",
    "    role, content = process_response(response_history)\n",
    "    chat_history.append({\"role\":role,\"content\":content})\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
